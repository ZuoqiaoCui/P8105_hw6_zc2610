---
title: "p8105_hw6_zc2610"
author: "Zuoqiao Cui"
date: "2022-11-22"
output: github_document
---

```{r}
library(tidyverse)
library(modelr)
library(mgcv)
```

### Problem 1

To obtain a distribution for $\hat{r}^2$, we'll follow basically the same procedure we used for regression coefficients: draw bootstrap samples; the a model to each; extract the value I'm concerned with; and summarize. Here, we'll use `modelr::bootstrap` to draw the samples and `broom::glance` to produce `r.squared` values. 

```{r weather_df, cache = TRUE}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```


```{r}
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::glance)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  ggplot(aes(x = r.squared)) + geom_density()
```

In this example, the $\hat{r}^2$ value is high, and the upper bound at 1 may be a cause for the generally skewed shape of the distribution. If we wanted to construct a confidence interval for $R^2$, we could take the 2.5% and 97.5% quantiles of the estimates across bootstrap samples. However, because the shape isn't symmetric, using the mean +/- 1.96 times the standard error probably wouldn't work well.

We can produce a distribution for $\log(\beta_0 * \beta1)$ using a similar approach, with a bit more wrangling before we make our plot.

```{r}
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::tidy)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  select(id = `.id`, term, estimate) %>% 
  pivot_wider(
    names_from = term, 
    values_from = estimate) %>% 
  rename(beta0 = `(Intercept)`, beta1 = tmin) %>% 
  mutate(log_b0b1 = log(beta0 * beta1)) %>% 
  ggplot(aes(x = log_b0b1)) + geom_density()
```

As with $r^2$, this distribution is somewhat skewed and has some outliers. 

The point of this is not to say you should always use the bootstrap -- it's possible to establish "large sample" distributions for strange parameters / values / summaries in a lot of cases, and those are great to have. But it is helpful to know that there's a way to do inference even in tough cases. 


# Problem 2

```{r}
homicide_data_df = read_csv("./data/homicide-data.csv") %>% 
janitor::clean_names()
```

Create a new variable "city_state" to combine city and state columns

```{r}
homicide_data_df = homicide_data_df %>% 
  mutate(
    city_state = str_c(city, state, sep = ",")
  ) %>% 
  select(-city,-state)
```

1. Create a new binary variable indicating whether the homicide is solved (0 means "unsolved" and 1 means "solved")

2. Omit cities Dallas, TX; Phoenix, AZ; Kansas City, MO and Tulsa, AL

3. Limit the analysis those for whom victim_race is white or black

4. Be sure victim_age is numeric

```{r}
homicide_data_df = homicide_data_df %>% 
  mutate(
    whether_solved = case_when(
      disposition == "Closed without arrest" ~ 0,
      disposition == "Open/No arrest" ~ 0,
      disposition == "Closed by arrest" ~ 1
    )
  ) %>% 
filter(
    city_state != "Dallas,TX" & city_state != "Phoenix,AZ" & city_state != "Kansas City,MO" & city_state != "Tulsa,AL"
  ) %>% 
    filter(victim_age != 'Unknown') %>% 
  mutate(victim_age = as.numeric(victim_age)) %>% 
  filter(victim_race == "White" | victim_race == "Black")
```

1. For the city of Baltimore, MD, use the glm function to fit a logistic regression with resolved vs unresolved as the outcome and victim age, sex and race as predictors 

2. Save the output of glm as an R object

3. Apply the broom::tidy to this object

```{r}
homicide_Baltimore_df = homicide_data_df %>% 
  filter(city_state == 'Baltimore,MD') 
Baltimore_glm = glm(whether_solved ~ victim_age + victim_sex + victim_race,data = homicide_Baltimore_df, family = binomial()) %>% 
  broom::tidy()
```

1. Obtain the estimate and confidence interval of the adjusted odds ratio for solving homicides

2. Compare male victims to female victims keeping all other variables fixed

```{r}
Baltimore_glm %>% 
  mutate(
    OR = exp(estimate),
    OR_lower = exp(estimate - std.error * 1.96),
    OR_upper = exp(estimate + std.error * 1.96)
        ) %>%
   select(term, log_OR = estimate, OR,OR_lower,OR_upper, p.value)

fit_logistic_sex = Baltimore_glm %>%
  filter(term == "victim_sexMale") %>% 
  mutate(
    OR = exp(estimate),
    OR_lower = exp(estimate - std.error * 1.96),
    OR_upper = exp(estimate + std.error * 1.96)
        ) %>%
   select(term, log_OR = estimate, OR,OR_lower,OR_upper, p.value)
```

Answer:

1. The estimate of the adjusted odds ratio for solving homicides comparing male victims to female victims is `r fit_logistic_sex$log_OR`

2. The confident interval is (`r fit_logistic_sex$OR_lower`,`r fit_logistic_sex$OR_upper`)



1. Run glm for each of the cities in the homicide dataset

2. Obtain regression results

3. Extract the adjusted odds ratio (and CI) for solving homicides

4. Compare male victims to female victims only

```{r}
fit_logistic_df = homicide_data_df %>% 
  select(victim_age,victim_race,victim_sex,city_state,whether_solved) %>% 
  nest(-city_state) %>% 
  mutate(
   models = map(.x = data, ~glm(whether_solved ~ victim_age + victim_sex + victim_race, family = binomial(link = logit), data = .x))
  ) %>% 
  mutate(
    results = map(models, broom::tidy)
  ) %>% 
  select(-data,-models) %>% 
  unnest(results) %>% 
   mutate(
    OR = exp(estimate),
    OR_lower = exp(estimate - std.error * 1.96),
    OR_upper = exp(estimate + std.error * 1.96)
        ) %>% 
  select(city_state,term,log_OR = estimate,OR,OR_lower,OR_upper)
fit_logistic_sex_df = fit_logistic_df %>% 
  filter(term == "victim_sexMale")
```

Create a plot that shows the estimated ORs and CIs for each city and organize cities according to estimated OR

```{r}
fit_logistic_sex_df %>% 
  mutate(city_state = fct_reorder(city_state, OR)) %>% 
  ggplot(aes(x = city_state, 
             y = OR, 
             ymin = OR_lower, 
             ymax = OR_upper,
             color = city_state
             )
  ) +
    geom_point() +
    geom_errorbar() +
    theme_bw() +
    labs(
      x = "City",
      y = "OR",
      title = "Odds ratios and CIs for solving homicides comparing gender",
      caption = "Homicide Data"
    ) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = rel(0.8)),plot.title = element_text(hjust = 0.5),legend.position = "none") 
  
```

Comments:

1. New York, NY has the lowest odds ratio.


2. For cities whose OR is smaller than 1, males have lower odds compared to females. For cities whose OR is greater than 1, male has greater odds compared to female.


3. The confidence intervals of some cities (such as Chicago, JL) are narrow so that we can be more confident about future predictions for these cities.


# Problem 3

Import dataset

```{r}
birthweight_df = read_csv("./data/birthweight.csv") %>% 
janitor::clean_names()
```

Convert numeric to factor

```{r}
birthweight_df = birthweight_df %>% 
 mutate(
    babysex = as.factor(babysex),
    frace = as.factor(frace),
    malform = as.factor(malform),
    mrace = as.factor(mrace)
  )
```

Check missing data

```{r}
num_of_missing = sum(!complete.cases(birthweight_df))

num_of_missing
```

Answer:

There is no missing data in this dataset

In my hypothesis, I choose the following variables as predictors that may affect the child's birthweight.

* bhead: baby’s head circumference at birth (centimeters)
* blength: baby’s length at birth (centimeters)
* delwt: mother’s weight at delivery (pounds)
* fincome: family monthly income (in hundreds, rounded)
* malform: presence of malformations that could affect weight (0 = absent, 1 = present)
* smoken: average number of cigarettes smoked per day during pregnancy
* wtgain: mother’s weight gain during pregnancy (pounds)

Propose a regression model for birthweight

Modeling process:

This model is based on a hypothesized structure for the factors that underly birthweight

1. I hypothesize that the baby’s head circumference at birth and baby’s length at birth should have direct relationship with the baby's birth weight, so I include these two variables into my model.

2. I hypothesize that variables related to the mother during the process of pregnant should make effects to the baby's birth weight, so I include delwt,malform,smoken and wtgain variables into my model.

3. Since I hypothesize that the family income will influence the life quality, emotion or nutrition obtained by the mother, therefore, I include fincome into my model.

```{r}
fit_birthweight_model = lm(bwt ~ bhead + blength + delwt + fincome + malform + smoken + wtgain, data = birthweight_df)
```

Make a plot of model residuals against fitted values

```{r}
residuals_fitted_plot = birthweight_df %>% 
   modelr::add_predictions(fit_birthweight_model) %>% 
   modelr::add_residuals(fit_birthweight_model) %>% 
  ggplot(aes(x = pred, y = resid)) +
  geom_point(alpha = .3) +
  labs(
    title = "Model Residuals vs Fitted Values",
    x = "Fitted Values",
    y = "Residuals",
    caption = "birthweight data"
  ) +
  theme(plot.title = element_text(hjust = 0.5))

residuals_fitted_plot
```

Fit the first model: using length at birth and gestational age as predictors

```{r}
model_1 = lm(bwt ~ blength + gaweeks, data = birthweight_df)
```

Fit the second model: using head circumference, length, sex, and all interactions (including the three-way interaction) between these

```{r}
model_2 = lm(bwt ~ bhead + blength + babysex + bhead*blength + bhead*babysex + blength*babysex + bhead*blength*babysex, data = birthweight_df)
```

1. Create train and test datasets

2. Fit candidate models above and assess prediction accuracy

3. Obtain RMSEs

```{r}
cv_df = 
  crossv_mc(birthweight_df, 100) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
    ) %>% 
  mutate(
    train_fit_birthweight_model = map(train, ~ lm(bwt ~ bhead + blength + delwt + fincome + malform + smoken +    wtgain, data = birthweight_df)),
    train_model_1 = map(train,~lm(bwt ~ blength + gaweeks, data = birthweight_df)),
    train_model_2 = map(train,~lm(bwt ~ bhead + blength + babysex + bhead*blength + bhead*babysex + blength*babysex + bhead*blength*babysex, data = birthweight_df))
  ) %>% 
  mutate(
    rmse_my_model = map2_dbl(train_fit_birthweight_model, test, ~rmse(model = .x, data = .y)),
    rmse_model_1 = map2_dbl( train_model_1, test, ~rmse(model = .x, data = .y)),
    rmse_model_2 = map2_dbl( train_model_2, test, ~rmse(model = .x, data = .y))
  )
```

1. Focused on RMSE as a way to compare these models

2. Make plots that show the distribution of RMSE values for each candidate model.

```{r}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "RMSE",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = RMSE, color = model)) + geom_violin() +
  labs(
      title = "The distribution of RMSE values for each candidate model",
      caption = "Birthweight Data"
    ) +
  theme(plot.title = element_text(hjust = 0.5),legend.position = "bottom") 
```

Summary:

Compared to other two models, the model selected by myself has the lowest RMSE so that it is fairly optimal. While the first model has the highest RMSE so that it makes a bad prediction to the baby's birth weight compared to other two models.






